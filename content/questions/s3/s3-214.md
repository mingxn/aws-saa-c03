---
title: "A company’s reporting system delivers hundreds of .csv files..."
draft: false
categories:
  - S3
  - Storage
domain: "Design High-Performing Architectures"
difficulty: "medium"
question: "A company’s reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache Parquet format and must store the files in a transformed data bucket. Which solution will fulfill these requirements with the LEAST development effort?"
options:
  A: "Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data. Use EMR File System (EMRFS) to write files to the transformed data bucket."
  B: "Create an AWS Glue crawler to discover the data. Create an AWS Glue extraffict, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step."
  C: "Use AWS Batch to create a job denition with Bash syntax to transform the data and output the data to the transformed data bucket. Use the job denition to submit a job. Specify an array job as the job type."
  D: "Create an AWS Lambda function to transform the data and output the data to the transformed data bucket. Configure an event notification for the S3 bucket. Specify the Lambda function as the destination for the event notification."
answer: "A"
explanation: ""
tags:
  - s3
  - storage
---

<!-- Question data is in frontmatter -->
