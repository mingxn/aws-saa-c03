---
title: "A company’s reporting system delivers hundreds of .csv files..."
draft: false
categories:
  - S3
  - Storage
domain: "Design High-Performing Architectures"
difficulty: "medium"
question: "A company’s reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache Parquet format and must store the files in a transformed data bucket. Which solution will fulfill these requirements with the LEAST development effort?"
options:
  A: "Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data. Use EMR File System (EMRFS) to write files to the transformed data bucket."
  B: "Create an AWS Glue crawler to discover the data. Create an AWS Glue extraffict, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step."
  C: "Use AWS Batch to create a job denition with Bash syntax to transform the data and output the data to the transformed data bucket. Use the job denition to submit a job. Specify an array job as the job type."
  D: "Create an AWS Lambda function to transform the data and output the data to the transformed data bucket. Configure an event notification for the S3 bucket. Specify the Lambda function as the destination for the event notification."
answer: "B"
explanation: "AWS Glue is a fully managed extraffict, transform, and load (ETL) service that makes it easy to prepare and load data for analysis. In this scenario: The AWS Glue crawler can automatically discover the schema of your data stored in Amazon S3, including the .csv files. The AWS Glue ETL job allows you to define the transformation logic easily. You can create a job using a visual interface or script in Python/Spark. You can specify the transformed data bucket as the output location for the ETL job."
tags:
  - s3
  - storage
---

<!-- Question data is in frontmatter -->
